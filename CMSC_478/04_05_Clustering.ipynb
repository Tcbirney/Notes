{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Clustering\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Unsupervised learning\n",
    "- Learning from unlabeled data\n",
    "\n",
    "Unsupervised learning techniques\n",
    "- dimensionality rduction: projecting high dimensionality data on to spaces with lower dimensionality \n",
    "\n",
    "- clustering: grouping similar instances together in clusters. \n",
    "\n",
    "- Anomalu Detection: Learning what normal data looks like. And then use that data to detect abnormal instances\n",
    "\n",
    "- density estimation: estimating probability density function "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Applications of clustering\n",
    "\n",
    "Customer segmentation based on online activity. Used in recommender system.\n",
    "\n",
    "Data analysis can benefit from this because we can cluster the data an then analyze each cluster seperately.\n",
    "\n",
    "Dimensionality Reduction can be a biproduct of clustering. Once a dataset has been clustered it is usually possible to measure each instances affinity to each cluster. Each instances feature vector x, can then possibly be replaced by a vector of cluster affinities. This affinity vector is usually of a much lower dimension space, but still preserved a lot of the information.\n",
    "\n",
    "Anomally Detection can appear because any instance with low affinity to all clusters is most likely to be an outlier. Can be used for online banking or networking errors.\n",
    "\n",
    "Semisupervised Learning. I fwe only have a few labels then we can perform clustering and then propegate the labels to all the instances in the cluster. This can increase the number of labels for any subsequent supervised classifier. Ued as a data preprocessing step to assign labels to the next supervised learning algorithm."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Similarity\n",
    "\n",
    "How do we measure it."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Clustering Approaches\n",
    "\n",
    "### Partitional Clustering\n",
    "\n",
    "#### K Means Clustering\n",
    "Based on centroid and euclidean distance metrics. Select K points randomly to be the centroid, then assign the points closest to that centroid to be a part of that cluster and recompute the centroid. Repeat until the centroid doesn't change. We usually compute closeness with euclidean distance, and mose centroids converge to a point within a few iterations.\n",
    "\n",
    "THe computational complexity is linear with repect the the number of instances m, number of clusters k, and number of dimensions n. Generally one of the fatest clustering algorithms.\n",
    "\n",
    "##### Centroid Initialization\n",
    "\n",
    "K means is guaranteed to converge, although it may not converge to the optimal solution, this depends on proper centroid Initialization. May converge to local optima rather than global optima\n",
    "\n",
    "_How do we avoid the uncertainty of centroid initialization?_\n",
    "\n",
    "Run the algorithm multiple times with different centroid initializations and keep the best solution, but how do we know which is the best. __With performance metrics__\n",
    "\n",
    "Inertia - The mean squared distance between each instance and its closest centroid. The bets model has the lowest inertia.\n",
    "\n",
    "N_init controls the number of random initializations. Defaults to 10. We can access the inertia with the inertia_ variable.\n",
    "\n",
    "Score returns the negative inertia, just because the score should always follow the convention that a higher score means a better model.\n",
    "\n",
    "#### K means ++\n",
    "\n",
    "Smarter initialization steps that have centroids that are more equally spaced apart. This makes it less likely to converge to a suboptimal solution.\n",
    "\n",
    "#### Accelerated Kmeans and Minibatch Kmeans\n",
    "\n",
    "Accelerated Kmeans is faster because it avoids many unneccessary distance calculations. This is the version of Kmeans that Scikit learn uses by default.\n",
    "\n",
    "Minibatch Kmeans uses random clusters of the dataset to alter the centroid positions slightly each time. It speeds up the algorithm be about 3 or 4. Kmeans has a limitation such that if the algorithm is too large it can take a very long time and we may not have engouh computation space. Minibatch has a very slightly higher inertia but is significantly faster to train.\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Sub Optimal Solutions with Bad Choices of Number of Clusters\n",
    "\n",
    "When the number of clusters is too small, seperate clusters get merged, but when the number of clusters is too high, a single cluster will get split.\n",
    "\n",
    "Inertia is not very good at determining K because it keeps getting lower as we increase K because we are always putting a new centroid which will be closer to any point. Even when plotting inertia vs K we cannot tell what number of K is appropriate.\n",
    "\n",
    "We will use the __Silhouette Score__\n",
    "- This computes $\\frac{b-a}{max(a, b)}$ over every instance, where a is the mean distance to the other intances in the same cluster, and b is the mean nearest cluster distance (ie the mean distance to the instances of the nearest cluster). The silhouette cluster can vary between -1 and + 1.\n",
    "\n",
    "When the silhouette distance is near +1 it means that the instance is well inside its own cluster and that it is far away from other clusters, and when it is -1, it means that it has been assigned to the wrong cluster"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Limitations to K means\n",
    "\n",
    "- It is necessary to run the algorithm multiple times to avoid suboptimal solutions\n",
    "- We need to specify the number of clusters\n",
    "- Does not be have when when the clusters have different densities or are not circular\n",
    "- Feature Scaling Is required to obtain a decent model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Applications to clustering\n",
    "\n",
    "### Image Segmentation\n",
    "\n",
    "In semantic segmentations all pixels that are a part of the same object type  are assigned to the same segment.\n",
    "\n",
    "In instance segmentation all pixels that are part of the same individual object are assigned to the same segment. Here we would have different segments for any individual pedestrian or other object.\n",
    "\n",
    "Done with convolutional neural networks.\n",
    "\n",
    "### Clustering for color Segmentation\n",
    "\n",
    "We assign pixels to the same segment if they have similar colors\n",
    "\n",
    "### Clustering For Prepreprocessing\n",
    "\n",
    "Can be used as a Prepreprocessing step for dimensionality reduction. It reduces the dimensions to the K different classes. We can also use label propegation for clusters to gain new insight and create a semi supervised dataset. \n",
    "\n",
    "### Active Learning\n",
    "Can be used to improve semisupervised learning. A human interacts with the learning algorithm and provides labels for specific instances when the algorithm requetss them to do so.\n",
    "\n",
    "One method of active learning is __Uncertainty Sampling__\n",
    "\n",
    "1. Model is trained on labeled instances gathered so ar, and model makes predictions on all the unlabeled instances.\n",
    "2. The instances for which the model is most uncertain are given to the expert to be labeled.\n",
    "3. You repeat this until the performance improvement stops being worth the labelling effort. \n",
    "\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## DBSCAN\n",
    "\n",
    "Another popular clustering algorithm that allows the model to recognize clusters of arbitrary shapes.\n",
    "\n",
    "For each instance the algorithm counts how many instances are located within a small distance $\\epsilon$ from it. This forms the area called the $\\epsilon$-neighborhood. If an instance has at least min_smaples instances in its $\\epsilon$-neighborhood including itself, then it is considered a core instance. Core instances are those that are contained in dense regions. All instances in the neighborhood of a ore instance belong to the same cluster. This may include other core instances which would then increase the area of the cluster.\n",
    "\n",
    "Any instance that is not a core instance is considered an anomaly. DBSCAN works well is the clusters are dense enough and if they are all separated by low density regions. If the density varies across the clusters it can be impossibel to capture all of the clusters appropriately. Computational complexity is O(mlog(m)), which is close to linear."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Heirarchical Clustering - Agglomerative Clustering\n",
    "\n",
    "Merge Very similar instances together and incrementally build larger clusters out of the smaller clusters. Iterate untill there is only one cluster left. This doesn not one clustering model, but multiple clustering models represented by a dendrogram. We can choose the number of clusters by picking which layer of the dedro gram we want to examine. As we go down the dedrogram we get more clusters and at the top there is only one cluster.\n",
    "\n",
    "How do we define closest for clusters with multiple elements.\n",
    "- Compare the distance of the closest pair of elements\n",
    "- Compare the distance of the farthes pair of elements\n",
    "- Compare the distance of the mean of distances of all elements in the clusters"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Gaussian Mixture Model\n",
    "\n",
    "Probabilistic model that assumes that the instances were generated from a mixture of several Gaussian distributions whose paramters are unknown. Instances generated form a single gaussian solution generally look like an ellipsoid. \n",
    "\n",
    "<div style=\"text-align: center\"> <img src=\"Pictures/gmm.png\"> </div>\n",
    "\n",
    "- Circles Represent Random Variables\n",
    "- Squares are fixed values\n",
    "- large rectangles are plates, which indicate that their contents are repeated multiple times.\n",
    "- the number in the corner of the plate indicate how many times their content will be repeated. So a plate containing a random variable will create m different random variables.\n",
    "- there are also $k$ means $\\mu^{(j)}$ and k covariance matrices $\\Sigma^{(j)}$\n",
    "- there is then one weight vector $\\Phi$ containing all weights from $\\Phi^1$ to $\\phi^k$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "We start by estimating $\\phi$ and all distribution paramters $\\mu^1$ to $\\mu^k$ and $\\Sigma^1$ to $\\Sigma^k$\n",
    "\n",
    "SKLearn has a gmm class that makes this easy. We can see if the GMM converged and how many iteratios it took to converge.\n",
    "\n",
    "We can check the cluster that each instance was assigned to (hard clustering) with predict() or we can check the probability that the class belongs to a certain cluster (soft clustering) with the predict_proba method."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Expectation Maximization Algorithm\n",
    "\n",
    "This algorithm is similar to the k means algorithm.\n",
    "- It initializes the cluster paramters randomly and repeats its two steps until the model converges.\n",
    "- It first assigns instances to cluster - __the expectation step__\n",
    "    - Attempts to estimate the weiths inside of $\\phi$ and all of the distribution paramters given the dataset X\n",
    "- then updates the clusters - __the maximization step__\n",
    "    - This is a generalization of the kmeans algorithm. Not only finds the origin of the clusters ($u^1$ to $u^k$), but also their size shape and orientation $\\Sigma$, as well as their relative weights $\\mu$ \n",
    "\n",
    "Expectation Maximization uses soft cluster assignments. For each instacne during the expectaiton step, the algorithm estimates the probability that an instance belongs to each cluster. \n",
    "\n",
    "Then during the maximization step, each cluster is updated using all the instances in the dataset, with each instance weighted by the estimated probablity that it belongs to that cluster. These probalities are called the __responsibilities__ of the clusters for the instances.  During the maximization step, each cluster will mostly be impacted by the instance it is most responsible for.\n",
    "\n",
    "Each cluster is associated with relative proportional weights $\\pi^i$\n",
    "\n",
    "Each cluster can be represented in 3d with the vector {$\\pi^k, \\mu^k, \\Sigma^k$} where $\\mu^k$ and $\\Sigma^k$ form the plane of which the gaussian distribution is formed on.\n",
    "\n",
    "The probability that instance $z^i$ belongs to class k is $P(z^i = k) = \\pi$\n",
    "- _How plausible a future outcome x is given the paramter values $\\theta$_\n",
    "\n",
    "Given that $x_i$ is form cluster k and $u_k$ and $\\Sigma_k$, the likelihood of seeing that instance given the gaussian paramters\n",
    "- _How plausible a set of paramter values are after the after the outcome of x is known_\n",
    "\n",
    "\n",
    "### Maximum likelihood Estimation\n",
    "\n",
    "Given a dataset x, a common task is often to Try to eliminate the most likely values for the model paramters. We must first find the values that maximize the likelihood function given x.\n",
    "\n",
    "If a prior probability distribution g over $\\theta$ exists then it is possible to take it into account by maximizing $L(\\theta |x)g(\\theta )$\n",
    "- This is called a Maximum A-Posteriori Estimation\n",
    "This can be viewed as a regularized version of mle\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Bayes Theorem\n",
    "\n",
    "$p(z|X)$ = posterior = $\\frac{\\textrm{likelihood} \\times \\textrm{prior}}{\\textrm{evidence}}$ = $\\frac{p(X|z)p(z)}{p(X)}$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### BIC and AIC\n",
    "\n",
    "We cannot use inertia or sillhouette score to select the appropriate number of clusrer. Not reliable when clusters are not spherical. Want to minimize BIC and AIC. Test on different number of clusters.\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}