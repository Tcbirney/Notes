{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Clustering\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Unsupervised learning\n",
    "- Learning from unlabeled data\n",
    "\n",
    "Unsupervised learning techniques\n",
    "- dimensionality rduction: projecting high dimensionality data on to spaces with lower dimensionality \n",
    "\n",
    "- clustering: grouping similar instances together in clusters. \n",
    "\n",
    "- Anomalu Detection: Learning what normal data looks like. And then use that data to detect abnormal instances\n",
    "\n",
    "- density estimation: estimating probability density function "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Applications of clustering\n",
    "\n",
    "Customer segmentation based on online activity. Used in recommender system.\n",
    "\n",
    "Data analysis can benefit from this because we can cluster the data an then analyze each cluster seperately.\n",
    "\n",
    "Dimensionality Reduction can be a biproduct of clustering. Once a dataset has been clustered it is usually possible to measure each instances affinity to each cluster. Each instances feature vector x, can then possibly be replaced by a vector of cluster affinities. This affinity vector is usually of a much lower dimension space, but still preserved a lot of the information.\n",
    "\n",
    "Anomally Detection can appear because any instance with low affinity to all clusters is most likely to be an outlier. Can be used for online banking or networking errors.\n",
    "\n",
    "Semisupervised Learning. I fwe only have a few labels then we can perform clustering and then propegate the labels to all the instances in the cluster. This can increase the number of labels for any subsequent supervised classifier. Ued as a data preprocessing step to assign labels to the next supervised learning algorithm."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Similarity\n",
    "\n",
    "How do we measure it."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Clustering Approaches\n",
    "\n",
    "### Partitional Clustering\n",
    "\n",
    "#### K Means Clustering\n",
    "Based on centroid and euclidean distance metrics. Select K points randomly to be the centroid, then assign the points closest to that centroid to be a part of that cluster and recompute the centroid. Repeat until the centroid doesn't change. We usually compute closeness with euclidean distance, and mose centroids converge to a point within a few iterations.\n",
    "\n",
    "THe computational complexity is linear with repect the the number of instances m, number of clusters k, and number of dimensions n. Generally one of the fatest clustering algorithms.\n",
    "\n",
    "##### Centroid Initialization\n",
    "\n",
    "K means is guaranteed to converge, although it may not converge to the optimal solution, this depends on proper centroid Initialization. May converge to local optima rather than global optima\n",
    "\n",
    "_How do we avoid the uncertainty of centroid initialization?_\n",
    "\n",
    "Run the algorithm multiple times with different centroid initializations and keep the best solution, but how do we know which is the best. __With performance metrics__\n",
    "\n",
    "Inertia - The mean squared distance between each instance and its closest centroid. The bets model has the lowest inertia.\n",
    "\n",
    "N_init controls the number of random initializations. Defaults to 10. We can access the inertia with the inertia_ variable.\n",
    "\n",
    "Score returns the negative inertia, just because the score should always follow the convention that a higher score means a better model.\n",
    "\n",
    "#### K means ++\n",
    "\n",
    "Smarter initialization steps that have centroids that are more equally spaced apart. This makes it less likely to converge to a suboptimal solution.\n",
    "\n",
    "#### Accelerated Kmeans and Minibatch Kmeans\n",
    "\n",
    "Accelerated Kmeans is faster because it avoids many unneccessary distance calculations. This is the version of Kmeans that Scikit learn uses by default.\n",
    "\n",
    "Minibatch Kmeans uses random clusters of the dataset to alter the centroid positions slightly each time. It speeds up the algorithm be about 3 or 4. Kmeans has a limitation such that if the algorithm is too large it can take a very long time and we may not have engouh computation space. Minibatch has a very slightly higher inertia but is significantly faster to train.\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Sub Optimal Solutions with Bad Choices of Number of Clusters\n",
    "\n",
    "When the number of clusters is too small, seperate clusters get merged, but when the number of clusters is too high, a single cluster will get split.\n",
    "\n",
    "Inertia is not very good at determining K because it keeps getting lower as we increase K because we are always putting a new centroid which will be closer to any point. Even when plotting inertia vs K we cannot tell what number of K is appropriate.\n",
    "\n",
    "We will use the __Silhouette Score__\n",
    "- This computes $\\frac{b-a}{max(a, b)}$ over every instance, where a is the mean distance to the other intances in the same cluster, and b is the mean nearest cluster distance (ie the mean distance to the instances of the nearest cluster). The silhouette cluster can vary between -1 and + 1.\n",
    "\n",
    "When the silhouette distance is near +1 it means that the instance is well inside its own cluster and that it is far away from other clusters, and when it is -1, it means that it has been assigned to the wrong cluster"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Limitations to K means\n",
    "\n",
    "- It is necessary to run the algorithm multiple times to avoid suboptimal solutions\n",
    "- We need to specify the number of clusters\n",
    "- Does not be have when when the clusters have different densities or are not circular\n",
    "- Feature Scaling Is required to obtain a decent model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Applications to clustering\n",
    "\n",
    "### Image Segmentation\n",
    "\n",
    "In semantic segmentations all pixels that are a part of the same object type  are assigned to the same segment.\n",
    "\n",
    "In instance segmentation all pixels that are part of the same individual object are assigned to the same segment. Here we would have different segments for any individual pedestrian or other object.\n",
    "\n",
    "Done with convolutional neural networks.\n",
    "\n",
    "### Clustering for color Segmentation\n",
    "\n",
    "We assign pixels to the same segment if they have similar colors\n",
    "\n",
    "### Clustering For Prepreprocessing\n",
    "\n",
    "Can be used as a Prepreprocessing step for dimensionality reduction. It reduces the dimensions to the K different classes. We can also use label propegation for clusters to gain new insight and create a semi supervised dataset. \n",
    "\n",
    "### Active Learning\n",
    "Can be used to improve semisupervised learning. A human interacts with the learning algorithm and provides labels for specific instances when the algorithm requetss them to do so.\n",
    "\n",
    "One method of active learning is __Uncertainty Sampling__\n",
    "\n",
    "1. Model is trained on labeled instances gathered so ar, and model makes predictions on all the unlabeled instances.\n",
    "2. The instances for which the model is most uncertain are given to the expert to be labeled.\n",
    "3. You repeat this until the performance improvement stops being worth the labelling effort. \n",
    "\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## DBSCAN\n",
    "\n",
    "Another popular clustering algorithm that allows the model to recognize clusters of arbitrary shapes.\n",
    "\n",
    "For each instance the algorithm counts how many instances are located within a small distance $\\epsilon$ from it. This forms the area called the $\\epsilon$-neighborhood. If an instance has at least min_smaples instances in its $\\epsilon$-neighborhood including itself, then it is considered a core instance. Core instances are those that are contained in dense regions. All instances in the neighborhood of a ore instance belong to the same cluster. This may include other core instances which would then increase the area of the cluster.\n",
    "\n",
    "Any instance that is not a core instance is considered an anomaly. DBSCAN works well is the clusters are dense enough and if they are all separated by low density regions. If the density varies across the clusters it can be impossibel to capture all of the clusters appropriately. Computational complexity is O(mlog(m)), which is close to linear."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Heirarchical Clustering - Agglomerative Clustering\n",
    "\n",
    "Merge Very similar instances together and incrementally build larger clusters out of the smaller clusters. Iterate untill there is only one cluster left. This doesn not one clustering model, but multiple clustering models represented by a dendrogram. We can choose the number of clusters by picking which layer of the dedro gram we want to examine. As we go down the dedrogram we get more clusters and at the top there is only one cluster.\n",
    "\n",
    "How do we define closest for clusters with multiple elements.\n",
    "- Compare the distance of the closest pair of elements\n",
    "- Compare the distance of the farthes pair of elements\n",
    "- Compare the distance of the mean of distances of all elements in the clusters"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Gaussian Mixture Model\n",
    "\n",
    "Probabilistic model that assumes that the instances were generated from a mixture of several Gaussian distributions whose paramters are unknown. Instances generated form a single gaussian solution generally look like an ellipsoid. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}