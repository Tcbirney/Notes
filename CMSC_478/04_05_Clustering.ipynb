{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Clustering\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Unsupervised learning\n",
    "- Learning from unlabeled data\n",
    "\n",
    "Unsupervised learning techniques\n",
    "- dimensionality rduction: projecting high dimensionality data on to spaces with lower dimensionality \n",
    "\n",
    "- clustering: grouping similar instances together in clusters. \n",
    "\n",
    "- Anomalu Detection: Learning what normal data looks like. And then use that data to detect abnormal instances\n",
    "\n",
    "- density estimation: estimating probability density function "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Applications of clustering\n",
    "\n",
    "Customer segmentation based on online activity. Used in recommender system.\n",
    "\n",
    "Data analysis can benefit from this because we can cluster the data an then analyze each cluster seperately.\n",
    "\n",
    "Dimensionality Reduction can be a biproduct of clustering. Once a dataset has been clustered it is usually possible to measure each instances affinity to each cluster. Each instances feature vector x, can then possibly be replaced by a vector of cluster affinities. This affinity vector is usually of a much lower dimension space, but still preserved a lot of the information.\n",
    "\n",
    "Anomally Detection can appear because any instance with low affinity to all clusters is most likely to be an outlier. Can be used for online banking or networking errors.\n",
    "\n",
    "Semisupervised Learning. I fwe only have a few labels then we can perform clustering and then propegate the labels to all the instances in the cluster. This can increase the number of labels for any subsequent supervised classifier. Ued as a data preprocessing step to assign labels to the next supervised learning algorithm."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Similarity\n",
    "\n",
    "How do we measure it."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Clustering Approaches\n",
    "\n",
    "### Partitional Clustering\n",
    "\n",
    "#### K Means Clustering\n",
    "Based on centroid and euclidean distance metrics. Select K points randomly to be the centroid, then assign the points closest to that centroid to be a part of that cluster and recompute the centroid. Repeat until the centroid doesn't change. We usually compute closeness with euclidean distance, and mose centroids converge to a point within a few iterations.\n",
    "\n",
    "THe computational complexity is linear with repect the the number of instances m, number of clusters k, and number of dimensions n. Generally one of the fatest clustering algorithms."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}