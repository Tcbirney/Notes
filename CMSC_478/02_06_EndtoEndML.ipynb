{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# End To End Machine Learning Projects"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## ML Project Checklist\n",
    "\n",
    "- [ ]  Look at the big picture. Specify problem and goal. \n",
    "- [ ]  Get Data\n",
    "- [ ]  Discover and visualize data to gain insights\n",
    "- [ ]  Prepare Data for machine learning algorithms\n",
    "- [ ]  Select a model and Train it\n",
    "- [ ]  Fine tune model\n",
    "- [ ]  Present Solution\n",
    "- [ ]  Launch, monitor, and maintain solution"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Some example machine learning datasets \n",
    "\n",
    "UC Irvine Machine learning repository\n",
    "\n",
    "Kaggle datasets\n",
    "\n",
    "Amazon's AWS datasets\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "__1. Cleaning Data__\n",
    "\n",
    "- Fix or remove outliers\n",
    "- Fill in missing values OR\n",
    "- Drop rows or columns for missing data (`pandas.DataFrame.dropna`)\n",
    "\n",
    "__2. Feature Selection __\n",
    "\n",
    "- Drop features that are less effective in creating a better model.\n",
    "\n",
    "__3. Feature Engineering__\n",
    "\n",
    "- Discretie continuous features. Spliting or containing the range of values of a feature.\n",
    "- Decomposing Features (Categorical, date/time, etc)\n",
    "- Add promising transmofrations of features (log(x), sqrt(x), x^2, etc)\n",
    "- Aggregate features in to promising new features\n",
    "\n",
    "\n",
    "__4. Feature Scaling__\n",
    "\n",
    "- Standardize or Normalize features\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    " ## Mathematical Notation\n",
    "\n",
    "$$\n",
    "\n",
    "\\textrm{Data instances are oftern represented as vectors}\\\\\n",
    "\n",
    "x = [x_1, x_2, \\dots , x_D]^\\intercal\\\\\n",
    "\n",
    "\n",
    "\\textrm{And the complete  data set $X$, is represented as a matrix}\\\\\n",
    "\n",
    "X = \n",
    "\n",
    "\\begin{bmatrix}\n",
    "x_{11} & x_{12} & \\dots & x_{1D}\\\\\n",
    "x_{11} & x_{12} & \\dots & x_{1D}\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "x_{N1} & x_{N2} & \\dots & x_{ND}\\\\\n",
    "\\end{bmatrix}\\\\\n",
    "\n",
    "\\textrm{There are also the output vector $y$ or $t$, as well as the model paramter vector $w$ }\\\\\n",
    "\n",
    "t = [t_1, t_1, \\dots ,  t_D]^\\intercal\\\\\n",
    "\n",
    "w = [w_1, w_1, \\dots ,  w_D]^\\intercal\\\\\n",
    "$$\n",
    "\n",
    "\n",
    "### Linear Model\n",
    "\n",
    "- Some models operate by taking a linear combination of input $X$ and weight $w$, i.e. the dot product of\n",
    "\n",
    "$$\n",
    "f(x:w) = w^\\intercal x = \\sum \\limits_0^D w_i x_i\\\\\n",
    "\n",
    "= w_0 x_0 + w_1 x_ 1 + \\dots + w_D x_D\\\\\n",
    "\n",
    "$$\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Scaling Features\n",
    "\n",
    "Some independent variables have different scales, and therefore can artifically cause some false sense of relevance to the change in said feature. We want to normalize the features to have similar scales so we can judge them fairly against one another. We can use __standardization__ for this. \n",
    "\n",
    "\n",
    "$$ \\tilde{x} = \\frac{x - \\bar{x}}{\\sigma} = \\frac{x - \\textrm{mean(x)}}{\\textrm{Std_dev(x)}}$$\n",
    "\n",
    "We can also use __Mean Normalization__, which normalized the feature based on its range.\n",
    "\n",
    "$$ \\tilde{x} = \\frac{x - \\bar{x}}{\\textrm{max(x) - minxc)}}$$\n",
    "\n",
    "\n",
    "We could also turn categorigal (low, middle, high), into a numberical values (0, 1, 2). Can to this with a `one_hot_encoder`\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Performance Measures\n",
    "\n",
    "Those performance measures used for classification are __different__ than those used for regression. You can't use MSE for classification, and you can't use recall for regression.\n",
    "\n",
    "The most common performance measures for regression are \n",
    "- Mean Squared Error\n",
    "- Root Mean Squared Error\n",
    "- Mean Absolute Error\n",
    "- $R^2$ Score\n",
    "\n",
    "The most common performance measures for classification are\n",
    "- Accuracy\n",
    "- Precision\n",
    "- Recall\n",
    "- F_1 Score\n",
    "- ROC Curve\n",
    "- Area Under Curve (AUC)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Data Partitioning\n",
    "\n",
    "## Training and Testing Set\n",
    "\n",
    "We need to split data sets into a larger (around 80%) data set, that the model can train on, and then a smaller dataset (whatever is left), which the model can't see the answers for and is used for evaluation. We can also use cross validation for a more robust evaluation method.\n",
    "\n",
    "\n",
    "Usually done with SCiKit Learns `train-test-split` function"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Fine Tuning Model Hyper Parameters\n",
    "\n",
    "Hyperparamters are those set before we even start training, and remain unchanged during each training session. They control and optimize model performance. Differ from model parameters in that model parameters are adjusted during training.\n",
    "\n",
    "In a neural network, we have neurons and edges connecting each leayer of neurons. Each of these edges have associated weights. In this example, the hyperparamters would be the number of neurons, or number of layers of neurons. So the major achritectural components of the model. Whereas the model paramters would be the weights of each edge."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Example: Califorina Housing Prices"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "In this example we want to predict housing prices in california based on  number of features. This is a regression problem, so we need to use a regression performance measure to evaluate the model.\n",
    "\n",
    "We can use Root Mean Square Error\n",
    "\n",
    "$$RMSE(X, h) = \\sqrt{ \\frac{1}{m} \\sum \\limits_{i=1}^m(h(x^{(i)} - y^{(i)})^2}$$\n",
    "\n",
    "This also implicitly shows us MSE, which is just RMSE without taking the square root at the end.\n",
    "\n",
    "Or we could use Mean Absolute Error\n",
    "\n",
    "$$MAE(X, h) = \\frac{1}{m} \\sum \\limits_{i=1}^m |h(x^{(i)}) - y^{(i)}|$$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}