{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Classification\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "This is a supervised task where the programm is asked to specify $k$ categories which an input belongs.\n",
    "\n",
    "The model does this by creating a function $f$ such that\n",
    "\n",
    "- $f: \\vec{X} \\rightarrow y$\n",
    "- Input: $x \\in IR^{d} = {X}$\n",
    "- Output: $y \\in \\{-1,\\ +1\\}$\n",
    "- Data: $D = \\{\\{x_1, y_1\\}, \\dots, (x_n, y_n)\\} \\textrm{ where } y_i = f(x_i)$\n",
    "\n",
    "This means that the model assigns an input described by vector $\\vec{X}$ to a category itentified by the numeric code $y\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Training and Testing\n",
    "\n",
    "\n",
    "We must first split our data into our inputs and outputs usualy by columns. The inputa are all of our features, and the outputs are the classes we are trying to predict. We split the data into a training set and a testing set. Usually around 70 to 80% of the data is reserved for training, while the rest is the portion used to test the model. \n",
    "\n",
    "Training is done by \n",
    "\n",
    "1. Fitting our model to the X_train inputs and the y_train outputs. `classifier = Model().fit(X_train, y_train)`\n",
    "\n",
    "2. Getting the model to predict what it thinks the correct outputs are in the domain of the X_test. `y_pred = classifier.predict(X_test)`\n",
    "\n",
    "3. Evaluating our model by comparing the y_pred and y_test. Y test represents the actual classes that map to X_test, and y_pred are the classes predicted by the model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Birnary Classification\n",
    "\n",
    "Classifying a data point as one of two classes or binary values. (yes, no), (poitive, negative), etc\n",
    "\n",
    "### Errors in Binary Classification\n",
    "\n",
    "A confusion matrix is something that can be used to judge the efficiency of a classification model. It compares the predicted value to the actual value for each class. The confusion matrix will count the total unmber of each of these types of judgements made by the model.\n",
    "\n",
    "- True Positve (TP): The model predicted that this instance __is__ a certain class, and it __is__. __Model is CORRECT__\n",
    "\n",
    "- False Positve (FP): The model predicted that this instance is a certain class, and it is __not__. __Model is INCORRECT__\n",
    "\n",
    "- False Negative (FN): The model predicted that this instance is __not__ a certain class, and it __is__, __Model is INCORRECT__\n",
    "\n",
    "- True Negative (TN): The model predicted that this instance is __not__ a certain class, and it is __not__. __Model is CORRECT__"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Performance Measures\n",
    "\n",
    "Accuracy: $\\frac{TP + TN}{TP + TN + FP + FN}$\n",
    "- This is how many it guessed correctly. Works well if the dataset is balanced. Similar number of positive to negative classes.\n",
    "\n",
    "Precision (TNP/Specificity): $\\frac{TP}{TP + FP}$\n",
    "- How many positive judgements were incorrect. The lower the number of False positives the better. This is good for something like spam filtering where we don't want to classify normal mail as spam. \n",
    "\n",
    "Recall (TPR/Sensitivity): $\\frac{TP}{TP + FN}$\n",
    "- Out of all of the positive classes, how many did we identify. The lower number of false negatives the better. This is important for something like medical diagnoses, where if can be life threatening if a positive class intance goes unlabeled.\n",
    "\n",
    "f1_score: $2 \\times \\frac{\\textrm{precision + recall}}{\\textrm{precision + recall}} = \\frac{TP}{TP + \\frac{FP + FN}{2}}$\n",
    "- This uses both precision and recall. Tries to get the average of the precision and the recall scores.\n",
    "\n",
    "\n",
    "### Precision Recall Trade-off\n",
    "\n",
    "There is a trade off between Precision and Recall. Trying to achieve more of one, decreases how much of the other you can have.\n",
    "\n",
    "<div style=\"text-align: center\"><img src=\"Pictures/pr_tradeoff.png\"></div>\n",
    "\n",
    "Classifiers have a threshold, where if the output of the classifier is above the threshold, it is treated as a positive class, and if it is below the threshold, then it is treated as a negative class. From the picture above, we can see that by lowering the threshold (decreasing the specificity and increasing the sensistivity) we increase the recall and decrease the precision. ANd vice versa if we increase the threshold.\n",
    "\n",
    "\n",
    "### ROC Curve and AUC \n",
    "\n",
    "The ROC curve plots the FPR against the TPR\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}