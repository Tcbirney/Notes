{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Decision Trees"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Making Predictions\n",
    "\n",
    "In order for decision trees to make the best prediction, they firts start at the root not of the tree and then ask yeas or no questions based on each of the features\n",
    "\n",
    "Each node of a tree has a samples attribute which tells how many training instances the nodes judgement parameter applies to. THe value attribute tells you how many training instances __of each class__ the node applies to. And finally a nodes Gini attribute tells you how pure the node is.\n",
    "\n",
    "- Gini Impurity \n",
    "\\begin{equation}\n",
    "G_i = 1 - \\sum_{k=1}^n {p_{i, k}}^2\\\\\n",
    "\\textrm{Where $P_{i, k}$ is the probablity of finding class $K$ in the node $i$}\\\\\n",
    "\\end{equation}\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## The CART Training Algorithm\n",
    "\n",
    "Scikit learn uses the CLassification and Regression Tree (CART) algorithm which only produces binary trees, where the non leaf nodes always have 2 children.\n",
    "\n",
    "THe algorithm splits the training data into 2 subsets using a single feature k and a threshold value $t_k$. It choses k and $t_k$ by choosing the pair that produces the purest subsets, weighted by their size.\n",
    "\n",
    "### Cart Cost Functions\n",
    "\n",
    "\\begin{equation}\n",
    "J(k, T_k) = \\frac{m_{left}}{m}G_{left} + \\frac{m_{right}}{m}G_{right}\\\\\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "Where $G_{left/right}$ is the imputiry of the left and right set and $m_{left/right}$ is the number of instances in the left and right set\n",
    "\n",
    "The CART algorithm splits each level recursively, diving into deeper levels for each node. It is a greedy algorithm, meanign it searches for the best plit at each level, with no heuristics for splits at deeper levels. Not guaranteed to find an optimal solution.\n",
    "\n",
    "### Computational Complexity\n",
    "Decision trees are approximately balanced, so traversing the decision tree takes $o(log_2(m))$ nodes, since they are plit in 2 at each node. However the training algorithm considers all features when splitting the node, so if we have n features, then the training complexity becomes, $O(m\\times n log_2(m))$\n",
    "\n",
    "## Gini or Entropy\n",
    "\n",
    "For decision trees, the Gini imputity measure is used by defualt. This can be changed by setting the criterion parameter.\n",
    "\\begin{equation}\n",
    "H_i = -\\sum_{k=1}^{n} p_{i, k}log_2(p_i, k)\n",
    "\\end{equation}"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Regularization Hyperparameters\n",
    "\n",
    "Unlike linear models, decision trees do not assume anything about the data. The tree structure will adapt to the shape of the data very closely, most likely overfitting it. This is called a non parametric model because the number of model parameters are not determined prior to training. A parametric model has a predefined number of parameters so its degree of freedom is limited, and therefore less likely to overfit the data.\n",
    "\n",
    "To avoid overfitting, we need to have some kind of regularization. Usually we can restrict maximum depth for regularization. We can also restrict how many samples a node must have to split or required to be a leaf."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Regression\n",
    "\n",
    "Decission Trees can perform regression with the DecisionTreeRegressor class. Instead of predicting a class it predicts a value for the output. \n",
    "\n",
    "Below is the CART cost function for Regression\n",
    "\\begin{equation}\n",
    "J(k, t_k) = \\frac{m_{left}}{m}MSE_{left} + \\frac{m_{right}}{m}MSE_{right}\\\\\n",
    "\\textrm{where}\\\\\n",
    "MSE_{node} = \\sum_{i \\in node}(\\hat{y}_{node} - y_i)^2\\\\\n",
    "\\textrm{and}\\\\\n",
    "\\hat{y}_{node} = \\frac{1}{m_{node}}\\sum_{i \\in node}y^i\\\\\n",
    "\\end{equation}\n",
    "\n",
    "THese are prone to overfitting when there is no regularization set."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Instability\n",
    "\n",
    "Decision trees overlook orthogonal boundaries. All splits are perpendicular to an axis.\n",
    "\n",
    "The main issue with decision trees is that they are very sensitive to small variations in the training data."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}