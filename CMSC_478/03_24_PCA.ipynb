{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Dimensionality Reduction and PCA"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Often times when we have very high dimensional feature space, our model can be prone to overfitting.\n",
    "\n",
    "Often dimes it is desirable to reduce the dimensionality of data for\n",
    "- data visualization\n",
    "- data exploration\n",
    "- efficient resource use\n",
    "- better generalization\n",
    "- noise removal\n",
    "- preprocessing\n",
    "\n",
    "The 2 main methods of doing so are \n",
    "- projection\n",
    "- manifold learning\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Projection\n",
    "\n",
    "If we want to reduce dimensionality we can create 2 new features what are projections of each feature on to the feature we want to remove. The new features are not exactly the same as the the original features.\n",
    "\n",
    "Porjection does not suit well for a lot of datasets. Like the swiss roll data set, where if we are to project the features onto one common feature, then we would gain no information and lse a lot of the relative information about the data points.\n",
    "\n",
    "## Manifold Learning\n",
    "\n",
    "We model the manifold that the training instances lie on. Relies on the manifold hypothesis which holds that most real world high dimensional datasets lie close to a much lower manifold. Accompanied with another implicit assumption that the task at hand will be simpler if its expressed in the lower dimensional space of the manifold\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Data Projection\n",
    "\n",
    "Recall that if\n",
    "\\begin{equation}\n",
    "x \\in R^D, ||w|| = 1\\\\\n",
    "\\end{equation}\n",
    "\n",
    "then $(w^Tx)w$ is the orthogonal projection of x onto w"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Dimensionality Reduction\n",
    "\n",
    "Dimensionality Reduction is a map definition problem.\n",
    "\n",
    "## Principal Component Analysis\n",
    "\n",
    "It is a data driven, unsupervised sample\n",
    "\n",
    "$$S = (x_1, \\dots, x_n)$$\n",
    "\n",
    "Where we want to derive a dimensionality reduction defined by a linear map M. PCA can be derived from several prospectives and here we give a geometric derivation.\n",
    "\n",
    "### PCA Problem Definition\n",
    "\n",
    "First consider k = 1, Then the associated reconstruction error is \n",
    "$$||x = (w^Tx)w||^2$$\n",
    "\n",
    "We want to find the direction P allowing the best reconstruction of the training set, ie minimal reconstruction error.\n",
    "\n",
    "Let $S^{D-1} = {w \\in R^D | ||w|| = 1}$ is the sphere in D dimensions. Then the the the empirical reconstruction  minimization problem is\n",
    "\\begin{equation}\n",
    "min_{w \\in S^{D-1}} \\frac{1}{n}\\sum_{i=1}^n ||x = (w^Tx)w||^2\n",
    "\\end{equation}\n",
    "\n",
    "And the solution p is known as the first Principal component. This is equivalent to \n",
    "\\begin{equation}\n",
    "max_{w \\in S^{D-1}} \\frac{1}{n}\\sum_{i=1}^n (w^T x_i)^2\n",
    "\\end{equation}"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Reconstruction and Variance\n",
    "\n",
    "We assume that the data has been centered around $\\bar{x} = \\frac{1}{n} x_i = 0$, then we can interpret the term $(w^Tx_i)^2$ as the variance of x in the direction of w. Thus the first principal component p is the direction along w which has the maximum vairance.\n",
    "\n",
    "### Centering\n",
    "\n",
    "If the data points are not centered we should consider\n",
    "\\begin{equation}\n",
    "max_{w \\in S^{D-1}} \\frac{1}{n}\\sum_{i=1}^n (w^T (x_i- \\bar{x}))^2\n",
    "\\end{equation}\n",
    "As beign equivalent to \n",
    "\\begin{equation}\n",
    "max_{w \\in S^{D-1}} \\frac{1}{n}\\sum_{i=1}^n (w^T (x_i^c))^2\n",
    "\\end{equation}\n",
    "\n",
    "With $x_i^c = (x_i - \\bar{x})$\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Projecting\n",
    "\n",
    "There are infite numbers of choices (directions) for Projecting the data, so we need criteria to pick them. We want to find the direction with minimum reconstruction errror, ie minimum variance loss. Equivalently, we can find the dorection with the maximum variance.\n",
    "\n",
    "### Preserving the variance\n",
    "\n",
    "Before you project the training set onto the lower dimensional hyperplane, you first need to choose the proper hyperplane. The hyperplane in this is in the direction of w?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Eigen Problem\n",
    "\n",
    "A further manipulation shows that the manipulation of the PCA corresponds to an eigen value problem.\n",
    "\n",
    "Using the symmetry of the inner product, we can see that\n",
    "\\begin{equation}\n",
    "\\frac{1}{n}\\sum_{i=1}^n (w^Tx_i)^2 = \n",
    "\\frac{1}{n}\\sum_{i=1}^n w^T x_i w^T x_i = \n",
    "\\frac{1}{n}\\sum_{i=1}^n w^T x_i x_i w =\n",
    "w^T (\\frac{1}{n}\\sum_{i=1}^n x_i x_i) w =\n",
    " \n",
    "\\end{equation}\n",
    "\n",
    "This leads to the problem\n",
    "$max_{w \\in S^{D-1}} w^T C_n w, C_n = \\frac{1}{n}\\sum_{i=1}^n x_i x_i^T$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Principal Components\n",
    "\n",
    "HOw do we find principal Components of a training set.\n",
    "\n",
    "With Singular Value Decomposition (SVD) that can decompose the training set matrix X into the matrix multiplication of three matrices $U \\Sigma V^T$ where V contains the unit vectors that define all the principal Components that we are looking for\n",
    "\n",
    "- U is an n by k orthogonal matrix\n",
    "- V i a D by k orthogonal matrix\n",
    "- $\\Sigma$ is a diagonal matix\n",
    "\n",
    "U and V are the left and right Singular vectors and diagonal entries of $\\Sigma$ the singular values."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## The Direction of Principal Components\n",
    "\n",
    "for each principal Component, PCA finds a zero centered unit vector pointing in the direction of the PC. Since there are two unit vectors in the same but opposite direction the direction of the selected unit vecor may flip, but it will be on the same axis as the other one.\n",
    "\n",
    "### Projecting down to D dimensions\n",
    "\n",
    "Once you have Identified all of the principal Components, you can reduce the dimensionality of the dataset down to d dimensions by projecting it onto the hyperplane define by the first d principal components.\n",
    "\n",
    "This hyperplane preserves as much variance as possible.\n",
    "$$X_{d_proj} = XW_d$$\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Explained Variance Ratio\n",
    "\n",
    "Indicates the proportion of the datasets variance that lies along each principal component. Can let us know when we have enough principal components. We can also plot the explained variance vs the number of dimensions and see when we get diminishing returns."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## PCA for compression\n",
    "\n",
    "We can also use PCA to compress data as much of the data is conserved with much fewer dimensional data points.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Randomized PCA\n",
    "\n",
    "We can set the svd_solver to be randomized, which will quickly find an approximation for the first D principal components.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Incremental PCA\n",
    "\n",
    "PCA must use the entire dataset to find principal components, so it does not scale as well as something like Incremental PCA where we split the training set into mini batches and fint the PCs one minibatch at a time.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Kernel PCA\n",
    "\n",
    "With PCA the implicit assumption is that the dataspace is in a linear subspace. We can use a kernel trick on this linear subspace dataset and have it perform complex non linear projections for dimensionality reduction.\n",
    "\n",
    "kPCA is good for preserving clusters of instances after a projection, or sometimes even unrolling datasets that lie close to a twisted manifold.\n",
    "\n",
    "Summary: kPCA makes it possible to perform complex non linear prpjections for dimensionality reductions\n",
    "\n",
    "\n",
    "### Selecting a Kernel and Tuning Hyper paramters\n",
    "\n",
    "As it is unsupervised there is no obvious performance measure to help you select the best kernel and hyper paramter values. Dimensionality reduction is often a preparation step for a supervised learning task, so we can use grid search to select the kernel and hyperparamters that lead to the best performance on that task."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## LLE\n",
    "\n",
    "Locally Linear Embedding\n",
    "\n",
    "This is a __nonlinear__ dimensionality reduction technique. It is a manifold learning technique that does not rely on projections like the previous algorithms do.\n",
    "\n",
    "It measures how far each training instance linearlly relates to its closet neighbors, then it looks for a low dimensional representation of the training set where those locla representaions are best percieved.\n",
    "\n",
    "Very good for unrolling manifolds, especially when there is low noise."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}