{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38564bitc5256a22b40043c38c55b2ccae26d546",
   "display_name": "Python 3.8.5 64-bit",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Regression"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Linear Regression"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "This is the simplest model to predict a continuous numerical value. There are two approaches \n",
    "\n",
    "- The closed form solution: Normal Equation (aka Least Squares)\n",
    "- The Optimal Solution: Least Mean Squares using Gradient Descent\n",
    "\n",
    "Mathematical Notation:\n",
    "$\\hat{y} = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 \\dots + \\theta_n x_n$\n",
    "\n",
    "Vith vectorized form\n",
    "\n",
    "$\\hat{y} = h_\\theta(x) = \\theta \\cdot x = \\theta ^ T x$ where $\\hat{y}$ represents the optimal solution and $x$ represents the input vector. This is linear because the weights in $\\theta_0$ are constant values."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Performance Measures\n",
    "\n",
    "We use the mean squared error perormance measure.\n",
    "\n",
    "$$MSE(X, h_\\theta) = \\frac{1}{m} \\sum\\limits_{i=1}^m (\\theta ^T x^i-y^i)^2 \\textrm{ where m is the number of instances}\\\\$$ \n",
    "We can see that in the parentheses we compute the error between the predicted output $\\theta_0 \\cdot x^{(1)}$ and the actual output $y^{(i)}$. The Compute the usum of the errors squared across all instances of x and then divide by the number of instances.\n",
    "\n",
    "\n",
    "There is also the mean absolute error performance measure which performs a similar computation, except it takes the absolute valus of the error instead of squaring it.\n",
    "\n",
    "$$MAE(X, h_\\theta) = \\frac{1}{m} \\sum\\limits_{i=1}^m |\\theta ^T x^i-y^i| \\textrm{ where m is the number of instances}\\\\$$\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Linear Regression - Closed Form Solution\n",
    "\n",
    "$\\hat{\\theta} = (X^TX)^{-1}X^Ty$\n",
    "\n",
    "Here, $\\hat{\\theta}$ is the value of $\\theta$ that minimizes the cost function, and $y$ is the vector of the target values containing $y^{(1)}$ to $y^{(m)}$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Nonlinear Regression - Polynomial Regression\n",
    "\n",
    "This recognizes non linear patters in datasets. To do this we add powers of each feature as new features, then we train a linear model on this extended set of features. This technique is called a __Polynomial regression__.\n",
    "\n",
    "When there are multiple features, polynomial regression allows us to find linear relations between feautes, unlike linear regression, because the PolynomialFeatures degree parameter defines the power of linear combinationsto create features for.\n",
    "\n",
    "High degree (around 300) polynomial regression will most likely fit the data better than plain linear regression."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Overfitting vs Underfitting\n",
    "\n",
    "We can overfit the data when the degree of the regression is too high, and we can also underfit data when the degree is too low.\n",
    "\n",
    "Overfitting: When the model performs well on training data, but generalizes poorly according to cross validation\n",
    "\n",
    "Underfitting: When the model performs poorly on training data and by cross validation metrics.\n",
    "\n",
    "Overfitting and Underfitting is a good way to see if the model is too simple or complex.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Learning Curves\n",
    "\n",
    "Learning curves are another way to tell if overfitting or underfitting occurs. Learning Curves are plots of the models performance with respect to the size of the training set. This way we can tell is the training set or the model size itself is the problem.\n",
    "\n",
    "Curves that show an underfitting model usually have error curves that reach close plateaus very early on, with high errors. __To resolve underfitting, you need to use better features or use a mode complex model__.\n",
    "\n",
    "\n",
    "Curves that show an overfitting model have the halmark sign of performance on the training data being significiantly better than on the validation data. On these, as we increase the overall size of the dataset, the lines would converge. __To resolve overfitting, we can either use more data, or use a simpler, lower degree model.__ We can use other techniques such as regularization to resolve overfitting.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Bias Variance Tradeoff\n",
    "\n",
    "Models generalizations error can be exptessed as the sum of 3 different errors.\n",
    "\n",
    "Bias: error due to wrong assumptions, such as assuming that the data is quadratic when it is linear. __High bias models are likely to underfit the data__\n",
    "\n",
    "Variance: error due to excessive sensitivity to small variations in the training data. __Models with many degrees of freedom (high polynomial models) have high variance and overfit data__\n",
    "\n",
    "Irreducible error: Error due to noisiness. Cleaning the data is the only solution."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Regularized Regression\n",
    "\n",
    "A method to avoid overfitting by constraining weights of the model parameters.The fewer degrees of freedom, the harder it is to overfit. For linear models, the way to regularize the expression is to limit the weights applied to the features.\n",
    "\n",
    "We study 3 Regularized versions of Linear Regressions\n",
    "- Lasso Regression\n",
    "- Ridge Regression\n",
    "- Elastic Net\n",
    "\n",
    "\n",
    "#### Ridge Regresssion (L2 Norm)\n",
    "\n",
    "\\begin{equation}\n",
    "J(\\theta) = MSE(\\theta) + \\alpha \\frac{1}{2}\\sum\\limits_{i=1}^n \\theta_i^2\\\\\n",
    "\\hat{\\theta} = (X^TX + \\alpha A)^{-1}X^Ty\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "#### Lasso Regresssion (L1 Norm)\n",
    "\\begin{equation}\n",
    "J(\\theta) = MSE(\\theta) + \\alpha \\frac{1}{2}\\sum\\limits_{i=1}^n |\\theta_i|\\\\\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "#### Elastic Net Regresssion (L1 and L2 Norm)\n",
    "\\begin{equation}\n",
    "\n",
    "J(\\theta) = MSE(\\theta) + \\alpha \\frac{1}{2}\\sum\\limits_{i=1}^n + \\alpha \\frac{1}{2}\\sum\\limits_{i=1}^n |\\theta_i|\\\\\n",
    "\\end{equation}"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Gradient Descent\n",
    "\n",
    "A generic optimization algorithms capable of finding optimal solutions to a wide range of of problems. The general idea od gradient descent is to tweak paramters iteratively in order to minimize a cost function.\n",
    "\n",
    "Measures the local gradient of the error function, with respect to the paramter vector $\\theta$ and it goes in the direction of t descending gradient. Once the gradient is 0, you have reached a minimum.\n",
    "\n",
    "Ransom initialization is when you initialize the paramter vector with random values, then you improve it gradually.\n",
    "\n",
    "Learning Step: the amount that we are allowed to change the paramter vector by. If it is too small, it will take too long to reach the optimal solution, but if it is too large, the it will bounce around and also not reach an optimal solution.\n",
    "\n",
    "Gradient descent can get caught in plateaus and local minimums.\n",
    "\n",
    "\n",
    "### Convex Functions\n",
    "\n",
    "The MSE cost functions are convex, which means that there are no local minimima, just ont global minima.\n",
    "\n",
    "Since gradient descent takes the gradient of the mean square error, we have \n",
    "\n",
    "\\begin{equation}\n",
    "MSE(X, h_\\theta) = \\frac{1}{m} \\sum\\limits_{i=1}^m (\\theta ^T x^i-y^i)^2\\\\\n",
    "\n",
    "\\frac{d}{d\\theta_i}MSE(\\theta) = \\frac{2}{m} \\sum\\limits_{i=1}^m (\\theta ^T x^i-y^i)x_j^i\\\\\n",
    "\n",
    "\\end{equation}"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Stochastic Gradient Descent\n",
    "The main problem with batch gradient descent is that it must use the whole training set to calculate the gradient at each step.\n",
    "\n",
    "The opposite is the Stochastic Gradient Descent which picks a random instance at the training set at every step and computes the gradient between these two points.\n",
    "\n",
    "### Minibatch Stochastic Gradient Descent\n",
    "Halfway between Gradient Descent and Stochastic Gradient Descent in that it computes the gradient on a random subset of data."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Early Stopping\n",
    "\n",
    "Early stopping is a way to regularize iterative learning algorithms such as Gradient Descent. We stop as soon as our validation error reaches A minimum."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Logistic Regression\n",
    "\n",
    "Used to estimate tbe probablility that an instance belongs to a class. If the eastimated probablility is greater than 0.5, then the model predicts that the instance elongs to the pisitive class. And otherwise it predicts that it belongs to the negative class.\n",
    "\n",
    "This maes linear regression a binary classifier.\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{p} = h_\\theta(x) = \\sigma(x^T\\theta) \\\\\n",
    "\\textrm{$\\sigma(x^T\\theta)$ is the sigmoid function predicts the probablitity of a positive class.}\\\\\n",
    "\\sigma(x) = \\frac{1}{1 + e^{-x}}\n",
    "\\end{equation}\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"248.518125pt\" version=\"1.1\" viewBox=\"0 0 372.103125 248.518125\" width=\"372.103125pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2021-03-02T14:22:43.170611</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.3.4, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 248.518125 \nL 372.103125 248.518125 \nL 372.103125 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 30.103125 224.64 \nL 364.903125 224.64 \nL 364.903125 7.2 \nL 30.103125 7.2 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"m76ae7b6d31\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"67.061567\" xlink:href=\"#m76ae7b6d31\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- −6 -->\n      <g transform=\"translate(59.690473 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 10.59375 35.5 \nL 73.1875 35.5 \nL 73.1875 27.203125 \nL 10.59375 27.203125 \nz\n\" id=\"DejaVuSans-8722\"/>\n        <path d=\"M 33.015625 40.375 \nQ 26.375 40.375 22.484375 35.828125 \nQ 18.609375 31.296875 18.609375 23.390625 \nQ 18.609375 15.53125 22.484375 10.953125 \nQ 26.375 6.390625 33.015625 6.390625 \nQ 39.65625 6.390625 43.53125 10.953125 \nQ 47.40625 15.53125 47.40625 23.390625 \nQ 47.40625 31.296875 43.53125 35.828125 \nQ 39.65625 40.375 33.015625 40.375 \nz\nM 52.59375 71.296875 \nL 52.59375 62.3125 \nQ 48.875 64.0625 45.09375 64.984375 \nQ 41.3125 65.921875 37.59375 65.921875 \nQ 27.828125 65.921875 22.671875 59.328125 \nQ 17.53125 52.734375 16.796875 39.40625 \nQ 19.671875 43.65625 24.015625 45.921875 \nQ 28.375 48.1875 33.59375 48.1875 \nQ 44.578125 48.1875 50.953125 41.515625 \nQ 57.328125 34.859375 57.328125 23.390625 \nQ 57.328125 12.15625 50.6875 5.359375 \nQ 44.046875 -1.421875 33.015625 -1.421875 \nQ 20.359375 -1.421875 13.671875 8.265625 \nQ 6.984375 17.96875 6.984375 36.375 \nQ 6.984375 53.65625 15.1875 63.9375 \nQ 23.390625 74.21875 37.203125 74.21875 \nQ 40.921875 74.21875 44.703125 73.484375 \nQ 48.484375 72.75 52.59375 71.296875 \nz\n\" id=\"DejaVuSans-54\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-8722\"/>\n       <use x=\"83.789062\" xlink:href=\"#DejaVuSans-54\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"110.542086\" xlink:href=\"#m76ae7b6d31\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- −4 -->\n      <g transform=\"translate(103.170992 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 37.796875 64.3125 \nL 12.890625 25.390625 \nL 37.796875 25.390625 \nz\nM 35.203125 72.90625 \nL 47.609375 72.90625 \nL 47.609375 25.390625 \nL 58.015625 25.390625 \nL 58.015625 17.1875 \nL 47.609375 17.1875 \nL 47.609375 0 \nL 37.796875 0 \nL 37.796875 17.1875 \nL 4.890625 17.1875 \nL 4.890625 26.703125 \nz\n\" id=\"DejaVuSans-52\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-8722\"/>\n       <use x=\"83.789062\" xlink:href=\"#DejaVuSans-52\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"154.022606\" xlink:href=\"#m76ae7b6d31\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- −2 -->\n      <g transform=\"translate(146.651512 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-8722\"/>\n       <use x=\"83.789062\" xlink:href=\"#DejaVuSans-50\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"197.503125\" xlink:href=\"#m76ae7b6d31\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 0 -->\n      <g transform=\"translate(194.321875 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"240.983644\" xlink:href=\"#m76ae7b6d31\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 2 -->\n      <g transform=\"translate(237.802394 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"284.464164\" xlink:href=\"#m76ae7b6d31\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 4 -->\n      <g transform=\"translate(281.282914 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-52\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_7\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"327.944683\" xlink:href=\"#m76ae7b6d31\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 6 -->\n      <g transform=\"translate(324.763433 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-54\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_8\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"mf74a685c46\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#mf74a685c46\" y=\"214.936782\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 0.0 -->\n      <g transform=\"translate(7.2 218.736001)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 10.6875 12.40625 \nL 21 12.40625 \nL 21 0 \nL 10.6875 0 \nz\n\" id=\"DejaVuSans-46\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#mf74a685c46\" y=\"175.330069\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 0.2 -->\n      <g transform=\"translate(7.2 179.129288)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-50\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#mf74a685c46\" y=\"135.723356\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 0.4 -->\n      <g transform=\"translate(7.2 139.522575)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-52\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#mf74a685c46\" y=\"96.116644\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 0.6 -->\n      <g transform=\"translate(7.2 99.915862)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-54\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#mf74a685c46\" y=\"56.509931\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 0.8 -->\n      <g transform=\"translate(7.2 60.309149)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 31.78125 34.625 \nQ 24.75 34.625 20.71875 30.859375 \nQ 16.703125 27.09375 16.703125 20.515625 \nQ 16.703125 13.921875 20.71875 10.15625 \nQ 24.75 6.390625 31.78125 6.390625 \nQ 38.8125 6.390625 42.859375 10.171875 \nQ 46.921875 13.96875 46.921875 20.515625 \nQ 46.921875 27.09375 42.890625 30.859375 \nQ 38.875 34.625 31.78125 34.625 \nz\nM 21.921875 38.8125 \nQ 15.578125 40.375 12.03125 44.71875 \nQ 8.5 49.078125 8.5 55.328125 \nQ 8.5 64.0625 14.71875 69.140625 \nQ 20.953125 74.21875 31.78125 74.21875 \nQ 42.671875 74.21875 48.875 69.140625 \nQ 55.078125 64.0625 55.078125 55.328125 \nQ 55.078125 49.078125 51.53125 44.71875 \nQ 48 40.375 41.703125 38.8125 \nQ 48.828125 37.15625 52.796875 32.3125 \nQ 56.78125 27.484375 56.78125 20.515625 \nQ 56.78125 9.90625 50.3125 4.234375 \nQ 43.84375 -1.421875 31.78125 -1.421875 \nQ 19.734375 -1.421875 13.25 4.234375 \nQ 6.78125 9.90625 6.78125 20.515625 \nQ 6.78125 27.484375 10.78125 32.3125 \nQ 14.796875 37.15625 21.921875 38.8125 \nz\nM 18.3125 54.390625 \nQ 18.3125 48.734375 21.84375 45.5625 \nQ 25.390625 42.390625 31.78125 42.390625 \nQ 38.140625 42.390625 41.71875 45.5625 \nQ 45.3125 48.734375 45.3125 54.390625 \nQ 45.3125 60.0625 41.71875 63.234375 \nQ 38.140625 66.40625 31.78125 66.40625 \nQ 25.390625 66.40625 21.84375 63.234375 \nQ 18.3125 60.0625 18.3125 54.390625 \nz\n\" id=\"DejaVuSans-56\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-56\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_13\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#mf74a685c46\" y=\"16.903218\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 1.0 -->\n      <g transform=\"translate(7.2 20.702436)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_14\">\n    <path clip-path=\"url(#pe15ba5745e)\" d=\"M 45.321307 214.756364 \nL 51.53281 214.696769 \nL 57.744312 214.617522 \nL 63.955815 214.512165 \nL 70.167318 214.372139 \nL 76.378821 214.186113 \nL 82.590324 213.939108 \nL 88.801826 213.611373 \nL 95.013329 213.17694 \nL 101.224832 212.6018 \nL 107.436335 211.841662 \nL 113.647837 210.839248 \nL 119.85934 209.521201 \nL 126.070843 207.794793 \nL 132.282346 205.544868 \nL 138.493849 202.631841 \nL 144.705351 198.892149 \nL 150.916854 194.143142 \nL 157.128357 188.195038 \nL 163.33986 180.872468 \nL 169.551362 172.046852 \nL 175.762865 161.677354 \nL 181.974368 149.852547 \nL 188.185871 136.818975 \nL 194.397374 122.980624 \nL 200.608876 108.859376 \nL 206.820379 95.021025 \nL 213.031882 81.987453 \nL 219.243385 70.162646 \nL 225.454888 59.793148 \nL 231.66639 50.967532 \nL 237.877893 43.644962 \nL 244.089396 37.696858 \nL 250.300899 32.947851 \nL 256.512401 29.208159 \nL 262.723904 26.295132 \nL 268.935407 24.045207 \nL 275.14691 22.318799 \nL 281.358413 21.000752 \nL 287.569915 19.998338 \nL 293.781418 19.2382 \nL 299.992921 18.66306 \nL 306.204424 18.228627 \nL 312.415926 17.900892 \nL 318.627429 17.653887 \nL 324.838932 17.467861 \nL 331.050435 17.327835 \nL 337.261938 17.222478 \nL 343.47344 17.143231 \nL 349.684943 17.083636 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 30.103125 224.64 \nL 30.103125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 364.903125 224.64 \nL 364.903125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 30.103125 224.64 \nL 364.903125 224.64 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 30.103125 7.2 \nL 364.903125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"pe15ba5745e\">\n   <rect height=\"217.44\" width=\"334.8\" x=\"30.103125\" y=\"7.2\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "x = np.linspace(-7, 7, 50)\n",
    "plt.plot(x, sigmoid(x));"
   ]
  },
  {
   "source": [
    "Note that as the the independent variable, $x^T\\theta$ or the weighted sum, deviates farthest from 0, the sigmoid function starts to converge the probability to either one or zero. The predicted probablitiy is denoted $\\hat{p}$\n",
    "\n",
    "The predicted class $\\hat{y}$ is 1 if $\\hat{p} \\ge 0.5$ and 0 otherwise.\n",
    "\n",
    "Or 1 if $x^T\\theta$ is positive, and 0 otherwise."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Logistic Regression Cost Function\n",
    "\n",
    "The cost function $c(\\theta) = -log(\\hat{p})$ if $y=1$ and $c(\\theta) = -log(1 - \\hat{p})$ if $y=0$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Softmax Regression\n",
    "\n",
    "Allows for a logistic regression model to classify between more than two classes without having to train a logistic regressor for each combination of classes. Also known as the multinomial regression.\n",
    "\n",
    "When given an instance x"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}