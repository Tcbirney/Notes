{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Neural Networks"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## The Perceptron\n",
    "\n",
    "One of the simplest ANN architectures\n",
    "\n",
    "It is based on another ANN neuron called the threshold logic unit. Input wiehgts are numbers, rather than binary on off values.And each connection has an associated weight.It computes the weighted sum of all imputs and then performs a step function to that weighted sum and determines an output.\n",
    "\n",
    "Most common step function is called the heavyside step function. Which outputs 0 if the input is < 0 and 1 otherwise. There is also the sign step function which will just output the sign of the input.\n",
    "\n",
    "A single TLU can be used to compute a linear binary classification. Unlike logistical regressors, they do not output a class probability. They only use hard thresholds. There are some limitations to perceptrons, for example, they cannot learn the XOR function.\n",
    "\n",
    "### Fully Connected Layer\n",
    "\n",
    "A fully conneted layer is when we have another layer in which every neuron in the first layer is connected to every neuron in the second layer. The second layeris considered a fully connected layer. The first layer is called the input layer. We also usually have a bias beuron which increases bias by always outputting 1.\n",
    "\n",
    "### Output of Fully Connected Layer\n",
    "\n",
    "$$h_{w, b}(X) = \\phi (XW + b)$$\n",
    "\n",
    "- X represents the matrix of input features, ie one row per instance, and one column per feature\n",
    "- W represents the connection weights, expect for the weight of the bias neuron. One row per input neuron, and one colum per artificial neuron in the layer\n",
    "- Bias vector B represents all the connections between the bias neuron and the artificial neurons. One bias term per artificial neuron.\n",
    "- $\\phi$ is called the activiation function."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Training Perceptron - Hebb's Rule\n",
    "\n",
    "When a biological neuron fires to another neuron, the connection between them grow stronger. __Cells that fire together wire together__\n",
    "\n",
    "The perceptron learning rule reinforces connections that help reduce error. The perceptron is fed one instance at a time, and for each instance it makes predictions. For every output neuron that produced a wrong prediction, it reinforces the connection wieghts from the inputs that would have contributed to the correct prediction\n",
    "\n",
    "$$w_{i, j}^{\\textrm{(next step)}} = w_{i, j} + n(y_j-\\hat{y}_j)$$\n",
    "\n",
    "- $w_{i, j}$ is the connection weight between the $i^{th}$ input neuron and the $j^{th}$ output neuron\n",
    "- $x_i$ is the $i^{th}$ input value of the current training instance\n",
    "- $\\hat{y}_j$ is the output of the $j_{th}$ output neuron for the current training instance\n",
    "- $y_j$ is the target out put for the $j_{th}$ output neuron for the current training instance\n",
    "- n is the learning rate\n",
    "\n",
    "The decision boundary of each output neuron layer is linear, so perceptrons are incapable of learning complex algorithms. Just like logiticall regression classifiers. However, if training instances are linearly seperable, Rosenblatt discovered that this algorithm would converge to a solution. This is called the perceptron convergence theorem."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Multilayer Perceptron\n",
    "\n",
    "The can solve the XOR problem. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Feedforward Neural Networks\n",
    "\n",
    "An MLP is composed of one Passthrough input layer, one or more layers of TLU, called hidden layers, and one final layer of TLUs called the output layer.\n",
    "\n",
    "Layers close to the input layer are called lower layers, and layers close to the output are called upper layers. Every layer except for the output layer contains a bias neuron.\n",
    "\n",
    "The signal flows in one direction, from the inputs to outputs so this is a feed forward neural network."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Epoch vs Iteration\n",
    "\n",
    "- Epoch: One full pass including forward pass and backwards pass of all training samples.\n",
    "- Batch Size: The number of training samples in one forwards/backwards pass.The higher the batch size, the more memory is used.\n",
    "- Iteration: Nnumber of Passes, each pass using batch size number of samples.\n",
    "\n",
    "If we have 1000 instances, and the batch size is 500, then it will take 20 iterations to complete 1 epoch (ie 1 full pass of all training instances) \n",
    "\n",
    "We need to randomly initialize all conection weights or else training will fail."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Deep Neural Networks\n",
    "\n",
    "When an NN contains a deep stack of hidden layers, it is called a deep neural network. Deep learning studies deep neural networks\n",
    "\n",
    "\n",
    "### Backpropegation Algorithms\n",
    "\n",
    "Generally it is a gradient descent algorithm using an efficient technique for computing the gradients automatically. In just 2 passes through the network (forwards and back) the propegation algorithm is able to compute the gradient of the network's error with regard to every single model paramter.\n",
    "\n",
    "It can find out how each connection weight and each bias term should be tweaked in order to reduce the error. Once it has these gradients it just performs a regular gradient descent step and the whole process is repeated until the network converges to the solution\n",
    "\n",
    "\n",
    "Back propegation handles one minibatch at a time, and goes through the training set multiple times. Each pass of all training records is called an epoch.\n",
    "\n",
    "Each minibatch is sent to the input layer which is passed to the first hidden layer. The oputput layer then computes the outputs for every instance in the minibatch. Going from the input layer to the output layer is alled the __forward pass__ . During the forward pass all intermediate predictions are preserved.\n",
    "\n",
    "Then the error is computed form the output layer and determines how much each output neuron contriuted to the error. This is done via the chain rule which makes computation fast and precise.\n",
    "\n",
    "Then the algorihtm moves a layer down (towards the input) and computes the error gradient for that layer of the neurons in that layer based on the intermediate prediction. Finally a gradient descent step is used to tweaks all connection weights using the error gradients it computed.\n",
    "\n",
    "The backpropegation algorithm needed to change the step funtion in the output layer to the signmoid function. This is because since the step function always provides outputs with a slope of 0, there was no nonzero gradient to work with , and therefore the gredience calculations wouldn't provide any useful information.\n",
    "\n",
    "There are also the hyperbolic tanget and the rectified linear unit activiation functions that work well for the backprogpegation algorithm.\n",
    "\n",
    "ReLU is accuracte and quick so it is the defualt"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## MLPs for Regression\n",
    "\n",
    "MPLs can be sued for regression. FOr exampl if we wanted to predict a single value, then we would just need the single output neuron.IT's output is the single predicted value.\n",
    "\n",
    "For multivariate regression (to predict multiple values at once), you need one output neuron per dimension.\n",
    "\n",
    "We do not want to use any activaion function for regression. We can regularize the output in some ways however. For example. RELU always regularizes the output to be positive. Or softplus, which is a smooth variant of RELU. Or if you want to make sure that the data stays within a certain range, you can use tanh or sigmoid.\n",
    "\n",
    "\n",
    "For classification, the hidden layers also have activation functions, usually ReLU."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## MLPs for Classification\n",
    "\n",
    "For a binary Classification we need just one output neuron with the logistic activation function that predicts a number bounded by 0 and 1, where 0 is the negative class and 1 is the positive class. 1 - that prediction is the estimated probability that the instance is of the negative class. \n",
    "\n",
    "For multiclass classification all we need to do is have an output neuron for each class. Then we can get the probability for each class determining how certain we are that an instance belongs to any combination of the classes.\n",
    "\n",
    "If an item can only occupy one class at a time then we must use softmax activation because then the probabilities of each output node will add to one\n",
    "\n",
    "Log loss is generally a good cost function for these classification algorithms since we are measuring the cross entropy loss."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Keras\n",
    "\n",
    "High Level deep learning API "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Tensor Flow\n",
    "\n",
    "Fit() returns a history object which is a dict containing loss and other extra metrics. If plotted then we can see that as training goes on, loss goes down and accuracy goes up\n",
    "\n",
    "\n",
    "### Wide and Deep Neural Network\n",
    "\n",
    "Non sequential neural network. It connects all or part of the input parameters straight to the output layer. This makes it possible for neural networks to learn more complicated (deep) rules as well as the simple (short) rules. \n",
    "\n",
    "\n",
    "### Saving and Restoring Models\n",
    "\n",
    "we call model.save after compiling and fitting it\n",
    "\n",
    "\n",
    "### Callbacks and checkpoints\n",
    "\n",
    "We can use call backs to save checkpoints t different intervals while training."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}