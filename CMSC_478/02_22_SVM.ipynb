{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Support Vector Machines"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Revisitng Binary Classification\n",
    "\n",
    "Given training data (x_i, y_i), for i = 1 to n, with X_i belonging to the real numbers any y ranging from -1 to 1, learn a classifier such that \n",
    "\n",
    "fx_i is > 0 if y = +1\n",
    "fx_i is < 1 0 if y = -1\n",
    "\n",
    "So y_ixf(x_i) > for a correct classification\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Linear separability \n",
    "means that a dataset contains two classes by a line. \n",
    "\n",
    "Linear CLassifier\n",
    "\n",
    "f(x) = w^T + by\n",
    "in 2d the discriminant is a line. W is the normal to the line, and b is the bias. W is known as the weight vector.\n",
    "\n",
    "N dimensional classifiers use a hyperplane to separate data. Uses the same firmula as the 2d classififier. In 3d it is a plane, and in higher dimensions it is a hyperplane."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Perceptron Algorithm\n",
    "\n",
    "Given a linearly separatable data x_i labeled into two cetegories, find a weight vector w such that the discriminant function f(x_i) = w^t + b. How do we find the hyperplane.\n",
    "\n",
    "Write the classifier as\n",
    "\n",
    "~w^T * ~x_i + w_0 = w^T x_i\n",
    "Where w = (~w, w0), x_i = (~x_i, 1)\n",
    "Initialize w =0\n",
    "Cycle through the data points, and if x_i is misclassified w = w + a sign(f(x_i))x_i, \n",
    "until all the data is correctly classified."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Finding the best hyperplane\n",
    "\n",
    "even if there are some violations, usually the hyperplane with the highest margin is the best solution. The margin is defined by 2 extra lines, w^Tx + B = -1 and w^Tx + b = 1. Points that lie on the margin are called support vectors.\n",
    "\n",
    "### Hard margins vs Soft margin\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Slack variables used for soft maring. COntrolled with the c hyper parameter.\n",
    "\n",
    "when xi is in between 0 and 1 then we have a correct classification \n",
    "\n",
    "impact of c on margin. Minimize norm of w + xi\n",
    "\n",
    "It is the weight of the error.SMallc makes margin violation easy to ignore, larger values of c make them harder to ignore, "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "SVM optimization\n",
    "\n",
    "Learning an vm is a csv\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "priaml and dual prblem\n",
    "\n",
    "the quadrtic optimization is the primal problem\n",
    "\n",
    "dual problem is solving as the sum over ai"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "kernel trick\n",
    "\n",
    "raising the dimensionality of data to make it so that it may be separated by a hyperplane\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}