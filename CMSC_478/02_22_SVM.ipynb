{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Support Vector Machines"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Revisitng Binary Classification\n",
    "\n",
    "Given training data (x_i, y_i), for i = 1 to n, with $X_i \\in R$ and $y \\in [-1, 1]$, learn a classifier such that \n",
    "\n",
    "$fx_i$ > 0 if y = +1\n",
    "$fx_i$ is < 1 0 if y = -1\n",
    "\n",
    "So $y_i \\times f(x_i)$ is positive for a correct classification\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Linear separability \n",
    "By definition linear separability means that a dataset contains two classes by a line. \n",
    "\n",
    "### Linear CLassifier\n",
    "\n",
    "$f(x) = w^T + by$\n",
    "\n",
    "In 2d the discriminant is a line. W is the normal to the line, and b is the bias. W is known as the weight vector.\n",
    "\n",
    "N dimensional classifiers use a hyperplane to separate data. They use the same formula as the 2d classififier. In 3d it is a plane, and in higher dimensions it is a hyperplane."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Perceptron Algorithm\n",
    "\n",
    "Given a linearly separatable data $x_i$ labeled into two categories, find a weight vector w such that the discriminant function $f(x_i) = w^T + b$. How do we find the hyperplane. Here th discriminant function describes the separating hyperplane.\n",
    "\n",
    "Write the classifier as\n",
    "\n",
    "$~w^T * ~x_i + w_0 = w^T x_i$\n",
    "\n",
    "Where $w = (~w, w0), x_i = (~x_i, 1)$\n",
    "\n",
    "Initialize w = 0\n",
    "\n",
    "Cycle through the data points, and if $x_i$ is misclassified, then we say $w = w + a sign(f(x_i))x_i$, \n",
    "until all the data is correctly classified.\n",
    "\n",
    "So here $\\alpha$ sort of acts like a learning rate?\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Finding the best hyperplane\n",
    "\n",
    "Even if there are some violations, usually the hyperplane with the highest margin is the best solution. The margin is defined by 2 extra lines, w^Tx + B = -1 and w^Tx + b = 1. The width of the mrgin is defined as $w = \\frac{2}{||w||}$Points that lie on the margin are called support vectors.\n",
    "\n",
    "### Hard margins vs Soft margin\n",
    "\n",
    "__Hard Margin__ - when we strictly impose that all instances must be off the margin and on the correct side\n",
    "- This only works when data is linearly seperable, and is very sensitiv to outliers\n",
    "\n",
    "__Soft Margin__ - when we want to find a good balance between Finding a large margin and reducing th numebr of margin violations (middle of street or even on the wrong side)\n",
    "\n",
    "For soft margins we introcude slack variable $\\xi(i)$ which measures how much the ith instance is allowed to go over the margin by. This introduces 2 conflicting objectives. The first ebing to reduce the slack variable to reduce margin violations, and the second being to reduce $\\frac{1}{2}w^Tw$ as to increase the margin.\n",
    "\n",
    "If $0 < \\xi(i) \\le 1$ the the point is correctly classified but in violation of the margin. If  $\\xi(i) > 1$ then we know that the point has been misclassified.\n",
    "\n",
    "The c hyperparamter can control the weight of the slack variables that we consider for soft margin classification. The larger the c, the more impactfull the margin enforcement becomes. It also controls the bias variance tradeoff we see in the classifier. The more margin errors we allow, then the higher the bias. If we strictly enforce hrd margins then the model may be prone to overfitting."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## The Dual Problem\n",
    "\n",
    "We know that for a learning a SVM linear classifier \n",
    "\n",
    "$$f(x) = w^Tx + b$$\n",
    "\n",
    "is formulated as solving an optimization problem over w:\n",
    "\\begin{equation}\n",
    "\\min||w||^2 + c\\sum^n_i\\max(0, 1 - y_if(x_i))\n",
    "\\end{equation}\n",
    "\n",
    "This quadratic problem is the primal problem. But we can also learn a linear svm classifier with \n",
    "\\begin{equation}\n",
    "f(x) = \\sum^N_i \\alpha_iy_i(x_i^Tx) + b\\\\\n",
    "\\end{equation}\n",
    "Which is an optimization over $\\alpha_i$, which is the dual problem\n",
    "\n",
    "\n",
    "### Representer Theorem\n",
    "W can always be written as a linear combination of the training data.\n",
    "$$w = \\sum^N_{j = 1}\\alpha_j y_j x_j$$\n",
    "\n",
    "Put this w back into the linear svm classifier $f(x)$ to get a problem in terms of $a_j$ rather than w which is better whne we have significantly fewer instances than we do dimensions of the data.\n",
    "\n",
    "Primal SVM: $f(x) = w^Tx + b$\n",
    "\n",
    "Dual SVM: $f(x) = \\sum_i^N \\alpha_i y_i(x_i^Tx) + b$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Kernel Trick\n",
    "\n",
    "How can we use SMVs for non linearly seperable data.\n",
    "\n",
    "The kernel trick transforms your data to a higher dimensional space, so that even though in the original feature space it is not linearly seperable, in that higher dimensional space it becomes linearly seperable by a hyperplane.\n",
    "\n",
    "It creates a new feature map (kernel) $\\phi(x)$ which is a higher dimensional representation of the original x. You don;t actually need to explicitly compute $\\phi(x)$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Online SVM\n",
    "\n",
    "Linear SVM classfiers can be done with gradient descent. SGD classifiers can be sued to minimze the cost function by using the primal problem. This converged much slower than quadratic optimization."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## SVM Regression\n",
    "\n",
    "We can use SVMs for regression as well as classification. In this process we want to reverse the opjective functions. Rather than maximizing the margins and classifying perfectly, we want to minimize tha margins while tring to fit as many instances within them. We can also pply the kernel trick to this method to achieve non linear SVM regression."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}