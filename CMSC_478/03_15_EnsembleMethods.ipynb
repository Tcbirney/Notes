{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Ensemble methods"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Ensemble methods are an easy way to create a better classifier from multiple classifiers. We aggregate the predictions of each classifier. they work the best when predictors are independent and make uncorrelated errors. We can get diverse classifiers by trining the data using different algorithms, which would increase the chance that they make uncorrelated errors.\n",
    "\n",
    "__Hardvoting__ - When we aggregate classifiers and pick the classification which gets the most votes among the aggregation. Gives higher accuracy than a single classifier. Uses the satistical mode to pick.\n",
    "\n",
    "If we have a classifier that can use `predictproba()` then we can predict the class with the highest probability.\n",
    "\n",
    "__Soft Voting__ - When we take the mean of the class averages and pick the class with the highest average. This often achieves higher performance than hard voting. Uses the arithmetic mean to pick."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Another way to get diverse classifiers is to use the same algorithm but train the classifier on random subsets of the training set.\n",
    "\n",
    "Bagging - when sampling is performed with replacement. Short for (bootstrap aggregation). No replacement means that samples don't get taken out of the bag when they get selected.\n",
    "\n",
    "Pasting - when sampling is performed without replacement. Replacement means that once an instance is selected for a single classifier, it gets taken out of that classifiers bag.\n",
    "\n",
    "Both of these methods allow for training instances to be sampled several times across the multiple predictors, but only bagging allow for training instances to be sampled multiple times for the same classifier.\n",
    "\n",
    "Trainging ensemble methods can be done in parallel, via different cores in the CPU. Predictions may also be made in parallel, which is one reason baggin and pasting is so popular.\n",
    "\n",
    "Since bagging introduces more variety, it will usually end up with higher bias and less variance.\n",
    "\n",
    "Bagging usually makes for a better predictor than pasting.\n",
    "\n",
    "Only about 63% of the training set is sampled for each classifier, but not the same 63%. The 37% are called instances. The classifiers never see the OOB (out of bag instances)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Random Forest\n",
    "\n",
    "An ensemble of decision trees is called a Random Forest. These are usually trained with the bagging method. We introduce extra randomness when growing trees becase when we split nodes, rather than searching for the best feature of all features to split a single node, it searches for the best feature among a random subset of features.\n",
    "\n",
    "This again yields higher bias and lower variance.\n",
    "\n",
    "We may also use random thresholds for the hyperparamters of splitting the node. A forest of extremelt random nodes is called an ensemble (extra-trees for short)\n",
    "\n",
    "Random forests also make it easy to measure the relative importance of each feature because SCIkit larn measures how much the tree nodes that use a feature to reduce impurity across all nodes in the forest. More specifically, the feature importance is a weighteed value with respect to how many training smaples are associated with it.\n",
    "\n",
    "The learning rate hyperparamter controls how much each tree contributes to the prediction. THe lower the learnign rate, the more trees you need to fit the training set. This is a regularization technique called shrinkage.\n",
    "\n",
    "We can also use early stopping to find the optimal number of trees."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Boosting\n",
    "\n",
    "A booster is any ensemble method that can take several weak learners into a strong learner. In boosters, we try to train predictors sequentially, and try to fix the errors of the previously trained learner. A popular one is the Adaptive Boost (ADA Boost)\n",
    "\n",
    "We pay attention to the training instances that were previously misclassfiied by its predecessor underfitted. We increase the weights of the misclassified trained instances, and this is done sequentially down the different classifiers. This is similar to gradient descent on that we weakt the paramters sequentially to find an optimal solution.\n",
    "\n",
    "These cannot be done sequentially as each clasfier needs to finish before we can see the instances it misclassified.\n",
    "\n",
    "We may use a gradient booster on a decision tree.\n",
    "\n",
    "\n",
    "\n",
    "There is also the extreme gradient booster. Rather than hard or soft voting, we train a model to learn the aggregation. We have the predictions made by each of the classifier, and then a blender takes them in and then makes the final rpediction. This ensures that the predicitons are clean, as the blender has never seen any of the training instances."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}